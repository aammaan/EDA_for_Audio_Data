{"cells":[{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:52:39.063577Z","iopub.status.busy":"2024-04-25T07:52:39.063205Z","iopub.status.idle":"2024-04-25T07:52:46.506233Z","shell.execute_reply":"2024-04-25T07:52:46.505370Z","shell.execute_reply.started":"2024-04-25T07:52:39.063547Z"},"trusted":true},"outputs":[],"source":["from transformers import BertTokenizer, BertForTokenClassification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from torch.utils.data import DataLoader, Dataset\n","import torch\n","import json\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:52:46.508767Z","iopub.status.busy":"2024-04-25T07:52:46.508207Z","iopub.status.idle":"2024-04-25T07:52:46.736223Z","shell.execute_reply":"2024-04-25T07:52:46.735407Z","shell.execute_reply.started":"2024-04-25T07:52:46.508732Z"},"trusted":true},"outputs":[],"source":["import json\n","with open('/home/hiddenmist/Aman_Lakshay/Pytorch_Profiler/Subtask_1_train.json', 'r') as file:\n","    data = json.load(file)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:52:46.737675Z","iopub.status.busy":"2024-04-25T07:52:46.737323Z","iopub.status.idle":"2024-04-25T07:52:46.755769Z","shell.execute_reply":"2024-04-25T07:52:46.754895Z","shell.execute_reply.started":"2024-04-25T07:52:46.737643Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1374\n"]}],"source":["conversation_list = []\n","\n","for i in data:\n","    \n","    temp = i['conversation']\n","    text_list = []\n","    for j in temp:\n","        text_list.append(j['text'])\n","    \n","    emotion_cause = i['emotion-cause_pairs']\n","    ec_dict = {}\n","    for j in emotion_cause:\n","        if j[0] not in ec_dict:\n","            ec_dict[j[0]] = [j[1]]\n","        else:\n","            ec_dict[j[0]].append(j[1])\n","            \n","    conversation_list.append({'text':text_list,'emotion_cause': ec_dict})\n","    \n","print(len(conversation_list))"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:52:46.758936Z","iopub.status.busy":"2024-04-25T07:52:46.758374Z","iopub.status.idle":"2024-04-25T07:52:46.765058Z","shell.execute_reply":"2024-04-25T07:52:46.764121Z","shell.execute_reply.started":"2024-04-25T07:52:46.758901Z"},"trusted":true},"outputs":[],"source":["def finder(list1, list2):\n","    i = 0\n","    j = 0\n","    while i<len(list1):\n","        if(list1[i]==list2[j]):\n","            j+=1\n","        else:\n","            j=0\n","        if(j == len(list2)):\n","            return i-j+1, i+1\n","        i+=1\n","    return -1, -1\n","\n","# print(finder([1,24,223,4,23,5,342,12,42],[223,4,23,5]))"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:52:46.766646Z","iopub.status.busy":"2024-04-25T07:52:46.766380Z","iopub.status.idle":"2024-04-25T07:52:47.034523Z","shell.execute_reply":"2024-04-25T07:52:47.033746Z","shell.execute_reply.started":"2024-04-25T07:52:46.766624Z"},"trusted":true},"outputs":[],"source":["emotion_list = ['anger','joy','fear','disgust','surprise','sadness','overall']\n","data_dict = {}\n","for i in emotion_list:\n","    data_dict[i] = []\n","    \n","for i in conversation_list:\n","    \n","    text_list = i['text']\n","    emotion_list = i['emotion_cause']\n","    \n","    for j in emotion_list:\n","        \n","        data_item = {}\n","        temp = j.split(\"_\")\n","        idx=int(j[0])\n","        emotion= temp[1]\n","        emotion_cause_list = emotion_list[j]\n","        \n","        text= []\n","        label = []\n","        \n","        for k in range(idx):\n","            temp_text = text_list[k].split()\n","            temp_label = [0]*len(temp_text)\n","            \n","            text.extend(temp_text)\n","            label.extend(temp_label)\n","            \n","        for k in emotion_cause_list:\n","            \n","            temp = k.split(\"_\")[1].split()\n","            s_idx, l_idx = finder(text, temp)\n","\n","            for x in range(s_idx,l_idx):\n","                label[x] = 1\n","\n","        data_item['text'] = text\n","        data_item['label'] = label\n","        data_dict[emotion].append(data_item)\n","        data_item['text'] = [emotion,':'] + text\n","        data_item['label'] = [1,0] + label\n","        data_dict['overall'].append(data_item)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:52:47.035956Z","iopub.status.busy":"2024-04-25T07:52:47.035634Z","iopub.status.idle":"2024-04-25T07:52:47.041001Z","shell.execute_reply":"2024-04-25T07:52:47.039912Z","shell.execute_reply.started":"2024-04-25T07:52:47.035929Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'text': ['surprise', ':', 'Alright', ',', 'so', 'I', 'am', 'back', 'in', 'high', 'school', ',', 'I', 'am', 'standing', 'in', 'the', 'middle', 'of', 'the', 'cafeteria', ',', 'and', 'I', 'realize', 'I', 'am', 'totally', 'naked', '.', 'Oh', ',', 'yeah', '.', 'Had', 'that', 'dream', '.', 'Then', 'I', 'look', 'down', ',', 'and', 'I', 'realize', 'there', 'is', 'a', 'phone', '...', 'there', '.'], 'label': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["for i in data_dict['surprise']:\n","    print(i)\n","    break"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:52:47.042410Z","iopub.status.busy":"2024-04-25T07:52:47.042130Z","iopub.status.idle":"2024-04-25T07:52:47.050964Z","shell.execute_reply":"2024-04-25T07:52:47.049979Z","shell.execute_reply.started":"2024-04-25T07:52:47.042388Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["anger\n","1373\n","joy\n","2093\n","fear\n","255\n","disgust\n","375\n","surprise\n","1624\n","sadness\n","1041\n","overall\n","6761\n"]}],"source":["emotion_list = ['anger','joy','fear','disgust','surprise','sadness','overall']\n","\n","for i in emotion_list:\n","    print(i)\n","    print(len(data_dict[i]))"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:52:47.052800Z","iopub.status.busy":"2024-04-25T07:52:47.052160Z","iopub.status.idle":"2024-04-25T07:52:47.061572Z","shell.execute_reply":"2024-04-25T07:52:47.060567Z","shell.execute_reply.started":"2024-04-25T07:52:47.052767Z"},"trusted":true},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        text = self.data[idx]['text']\n","        label = self.data[idx]['label']\n","        label = label + [0]*(512-len(label))\n","        text_str = \"\"   \n","        for k in text:\n","            text_str+=k\n","            text_str+=\" \"\n","        inputs = self.tokenizer(text_str, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n","        inputs['input_ids'] = inputs['input_ids'].reshape([512])\n","        inputs['token_type_ids'] = inputs['token_type_ids'].reshape([512])\n","        inputs['attention_mask'] = inputs['attention_mask'].reshape([512])\n","        \n","        return [inputs, torch.tensor(label, dtype=torch.long)]"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:52:47.063120Z","iopub.status.busy":"2024-04-25T07:52:47.062830Z","iopub.status.idle":"2024-04-25T07:52:48.586577Z","shell.execute_reply":"2024-04-25T07:52:48.585815Z","shell.execute_reply.started":"2024-04-25T07:52:47.063097Z"},"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:52:48.589448Z","iopub.status.busy":"2024-04-25T07:52:48.589150Z","iopub.status.idle":"2024-04-25T07:52:48.607204Z","shell.execute_reply":"2024-04-25T07:52:48.606264Z","shell.execute_reply.started":"2024-04-25T07:52:48.589423Z"},"trusted":true},"outputs":[],"source":["train_loader_dict = {}\n","val_loader_dict = {}\n","\n","for i in emotion_list:\n","    train_data, val_data = train_test_split(data_dict[i], test_size=0.2, random_state=42)\n","    train_loader_dict[i] = DataLoader(CustomDataset(train_data, tokenizer), batch_size=16, shuffle=True)\n","    val_loader_dict[i]= DataLoader(CustomDataset(val_data, tokenizer), batch_size=2, shuffle=False)"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:52:48.608979Z","iopub.status.busy":"2024-04-25T07:52:48.608688Z","iopub.status.idle":"2024-04-25T07:52:55.664905Z","shell.execute_reply":"2024-04-25T07:52:55.664031Z","shell.execute_reply.started":"2024-04-25T07:52:48.608955Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["learning_rate = 2e-5\n","emotion_list = ['anger','joy','fear','disgust','surprise','sadness','overall']\n","model_dict = {}\n","optimizer_dict = {}\n","for i in emotion_list:\n","    temp_model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=2).to(device)\n","    temp_optimizer = optimizer = torch.optim.AdamW(temp_model.parameters(), lr=learning_rate)\n","    model_dict[i] = temp_model\n","    optimizer_dict[i] = temp_optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:59:26.057682Z","iopub.status.busy":"2024-04-25T07:59:26.056796Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score\n","model = model_dict['anger'] \n","optimizer = optimizer_dict['anger']\n","train_loader = train_loader_dict['anger']\n","val_loader = val_loader_dict['anger']\n","                             \n","max_f1 = 0\n","\n","epochs = 30\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","    \n","    for batch in train_loader:\n","        inputs = batch[0].to(device)\n","        labels = batch[1].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(**inputs, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","    \n","    avg_train_loss = total_loss / len(train_loader)\n","    print(f'Epoch {epoch+1}/{epochs}, Average Training Loss: {avg_train_loss:.4f}')\n","\n","    # Validation loop\n","    model.eval()\n","    val_loss = 0\n","    correct = 0\n","    total = 0\n","    collated_total = []\n","    predicted_total = []\n","\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            inputs = batch[0].to(device)\n","            labels = batch[1].to(device)\n","            outputs = model(**inputs, labels=labels)\n","            loss, scores = outputs[:2]\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","            predicted = outputs.logits.argmax(dim=-1)\n","            collated_total.extend(labels.flatten().tolist())\n","            predicted_total.extend(predicted.flatten().tolist())\n","\n","    avg_val_loss = val_loss / len(val_loader)\n","    f1 = f1_score(collated_total, predicted_total)\n","    if(f1>max_f1):\n","        max_f1 = f1\n","        torch.save(model.state_dict(), 'overall_model.pth')\n","    print(f'Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}, F1: {f1}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-25T07:53:16.523484Z","iopub.status.idle":"2024-04-25T07:53:16.523884Z","shell.execute_reply":"2024-04-25T07:53:16.523677Z","shell.execute_reply.started":"2024-04-25T07:53:16.523663Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=2).to(device)\n","state_dict = torch.load('/home/hiddenmist/Aman_Lakshay/Pytorch_Profiler/overall_model.pth')\n","model.load_state_dict(state_dict)\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-25T07:53:16.525460Z","iopub.status.idle":"2024-04-25T07:53:16.525843Z","shell.execute_reply":"2024-04-25T07:53:16.525648Z","shell.execute_reply.started":"2024-04-25T07:53:16.525634Z"},"trusted":true},"outputs":[],"source":["def text_tokenizer(text, label, tokenizer):\n","    label = label + [0]*(512-len(label))\n","    text_str = \"\"   \n","    for k in text:\n","        text_str+=k\n","        text_str+=\" \"\n","    inputs = tokenizer(text_str, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n","    return [inputs, torch.tensor(label, dtype=torch.long)]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-25T07:53:16.527123Z","iopub.status.idle":"2024-04-25T07:53:16.527509Z","shell.execute_reply":"2024-04-25T07:53:16.527332Z","shell.execute_reply.started":"2024-04-25T07:53:16.527316Z"},"trusted":true},"outputs":[],"source":["def idx_returner(list1):\n","    ans = []\n","    s_idx = -1\n","    l_idx = -1\n","    for i, item in enumerate(list1):\n","        if item==1 and s_idx==-1:\n","            s_idx = i\n","        if item==1 and s_idx!=-1:\n","            l_idx = i\n","        if item==0 and l_idx!=-1:\n","            ans.append([s_idx+2,l_idx+2])\n","            s_idx=-1\n","            l_idx=-1\n","    if(s_idx!=-1 and l_idx!=-1):\n","        ans.append([s_idx+2,l_idx+2])\n","    print(ans)\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-25T07:53:16.529340Z","iopub.status.idle":"2024-04-25T07:53:16.529752Z","shell.execute_reply":"2024-04-25T07:53:16.529552Z","shell.execute_reply.started":"2024-04-25T07:53:16.529536Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","[[2, 10]]\n"]}],"source":["for i in data_dict['overall'][52:]:\n","    text_list = i['text']\n","    true_label = i['label']\n","    inputs, _ = text_tokenizer(text_list, true_label, tokenizer)\n","    inputs = inputs.to(device)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        predicted = outputs.logits.argmax(dim=-1)\n","    final_predicted = predicted.flatten().tolist()[:len(text_list)]\n","    print(final_predicted)\n","    print(true_label)\n","    emotion = text_list[0]\n","    idx_returner(final_predicted[2:])\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4861104,"sourceId":8204601,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelInstanceId":31635,"sourceId":37638,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"CROSSCAPS","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]"},"vscode":{"interpreter":{"hash":"85ee25f3913e79dfaf73cc9743a698525d72dde364ea63726e0c3f70211b2c45"}}},"nbformat":4,"nbformat_minor":4}
